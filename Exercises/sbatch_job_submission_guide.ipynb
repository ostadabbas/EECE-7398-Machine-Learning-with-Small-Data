{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Job Submission with sbatch on Explorer Cluster\n",
    "\n",
    "This notebook teaches you how to create and submit batch jobs using `sbatch` for long-running machine learning tasks on the Explorer cluster.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Understanding sbatch](#understanding-sbatch)\n",
    "2. [Basic Job Scripts](#basic-job-scripts)\n",
    "3. [GPU Job Scripts](#gpu-job-scripts)\n",
    "4. [Advanced Configurations](#advanced-configurations)\n",
    "5. [Job Arrays](#job-arrays)\n",
    "6. [Best Practices](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding sbatch <a name=\"understanding-sbatch\"></a>\n",
    "\n",
    "The `sbatch` command submits **batch jobs** to run without user interaction. Perfect for:\n",
    "- Long training runs\n",
    "- Parameter sweeps\n",
    "- Production workflows\n",
    "- Automated experiments\n",
    "\n",
    "### Key Benefits:\n",
    "- Runs without your presence\n",
    "- Better resource utilization\n",
    "- Automatic output logging\n",
    "- Can run multiple jobs simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Job Scripts <a name=\"basic-job-scripts\"></a>\n",
    "\n",
    "### 2.1 Simple CPU Job\n",
    "\n",
    "Create a basic job script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple job script\n",
    "simple_job_script = '''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=simple_job\n",
    "#SBATCH --partition=short\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=16G\n",
    "#SBATCH --time=02:00:00\n",
    "#SBATCH --output=simple_job_%j.out\n",
    "#SBATCH --error=simple_job_%j.err\n",
    "\n",
    "# Load modules\n",
    "module load anaconda3\n",
    "\n",
    "# Activate environment\n",
    "conda activate ml_course_env\n",
    "\n",
    "# Run your script\n",
    "python my_script.py\n",
    "'''\n",
    "\n",
    "# Save the script\n",
    "with open('simple_job.sh', 'w') as f:\n",
    "    f.write(simple_job_script)\n",
    "\n",
    "print(\"Created simple_job.sh\")\n",
    "print(\"Submit with: sbatch simple_job.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Job Scripts <a name=\"gpu-job-scripts\"></a>\n",
    "\n",
    "### 3.1 Single GPU Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU training job script\n",
    "gpu_job_script = '''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=gpu_training\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --time=08:00:00\n",
    "#SBATCH --output=gpu_training_%j.out\n",
    "#SBATCH --error=gpu_training_%j.err\n",
    "#SBATCH --mail-type=BEGIN,END,FAIL\n",
    "#SBATCH --mail-user=your.email@northeastern.edu\n",
    "\n",
    "# Print job info\n",
    "echo \"Job ID: $SLURM_JOB_ID\"\n",
    "echo \"Node: $SLURM_NODELIST\"\n",
    "echo \"GPU: $CUDA_VISIBLE_DEVICES\"\n",
    "echo \"Start time: $(date)\"\n",
    "\n",
    "# Load modules\n",
    "module load anaconda3\n",
    "module load cuda/11.8\n",
    "\n",
    "# Activate environment\n",
    "conda activate ml_course_env\n",
    "\n",
    "# Verify GPU\n",
    "nvidia-smi\n",
    "\n",
    "# Run training\n",
    "python train_model.py --epochs 100 --batch-size 64 --lr 0.001\n",
    "\n",
    "echo \"End time: $(date)\"\n",
    "'''\n",
    "\n",
    "with open('gpu_training.sh', 'w') as f:\n",
    "    f.write(gpu_job_script)\n",
    "\n",
    "print(\"Created gpu_training.sh\")\n",
    "print(\"Submit with: sbatch gpu_training.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-GPU Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU training script\n",
    "multi_gpu_script = '''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=multi_gpu_training\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --gres=gpu:2\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem=64G\n",
    "#SBATCH --time=12:00:00\n",
    "#SBATCH --output=multi_gpu_%j.out\n",
    "#SBATCH --error=multi_gpu_%j.err\n",
    "\n",
    "# Load modules\n",
    "module load anaconda3\n",
    "module load cuda/11.8\n",
    "\n",
    "# Activate environment\n",
    "conda activate ml_course_env\n",
    "\n",
    "# Check available GPUs\n",
    "echo \"Available GPUs: $CUDA_VISIBLE_DEVICES\"\n",
    "nvidia-smi\n",
    "\n",
    "# Run multi-GPU training\n",
    "python -m torch.distributed.launch --nproc_per_node=2 train_distributed.py\n",
    "'''\n",
    "\n",
    "with open('multi_gpu_training.sh', 'w') as f:\n",
    "    f.write(multi_gpu_script)\n",
    "\n",
    "print(\"Created multi_gpu_training.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Configurations <a name=\"advanced-configurations\"></a>\n",
    "\n",
    "### 4.1 Parameter Sweep Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter sweep job\n",
    "param_sweep_script = '''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=param_sweep\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --output=param_sweep_%j.out\n",
    "#SBATCH --error=param_sweep_%j.err\n",
    "\n",
    "# Load environment\n",
    "module load anaconda3\n",
    "conda activate ml_course_env\n",
    "\n",
    "# Parameter sweep\n",
    "learning_rates=(0.001 0.01 0.1)\n",
    "batch_sizes=(32 64 128)\n",
    "\n",
    "for lr in \"${learning_rates[@]}\"; do\n",
    "    for bs in \"${batch_sizes[@]}\"; do\n",
    "        echo \"Training with lr=$lr, batch_size=$bs\"\n",
    "        python train_model.py --lr $lr --batch-size $bs --experiment-name \"lr_${lr}_bs_${bs}\"\n",
    "    done\n",
    "done\n",
    "'''\n",
    "\n",
    "with open('param_sweep.sh', 'w') as f:\n",
    "    f.write(param_sweep_script)\n",
    "\n",
    "print(\"Created param_sweep.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Job Arrays <a name=\"job-arrays\"></a>\n",
    "\n",
    "Job arrays run multiple similar jobs efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job array script\n",
    "job_array_script = '''\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=job_array\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --array=1-10\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=32G\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --output=job_array_%A_%a.out\n",
    "#SBATCH --error=job_array_%A_%a.err\n",
    "\n",
    "# Load environment\n",
    "module load anaconda3\n",
    "conda activate ml_course_env\n",
    "\n",
    "# Use array task ID for different seeds/configurations\n",
    "SEED=$SLURM_ARRAY_TASK_ID\n",
    "\n",
    "echo \"Running job array task $SLURM_ARRAY_TASK_ID with seed $SEED\"\n",
    "\n",
    "# Run with different random seeds\n",
    "python train_model.py --seed $SEED --experiment-name \"run_$SEED\"\n",
    "'''\n",
    "\n",
    "with open('job_array.sh', 'w') as f:\n",
    "    f.write(job_array_script)\n",
    "\n",
    "print(\"Created job_array.sh\")\n",
    "print(\"This will submit 10 jobs with different seeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices <a name=\"best-practices\"></a>\n",
    "\n",
    "### 6.1 Resource Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource estimation guidelines\n",
    "resource_guide = \"\"\"\n",
    "Memory Guidelines:\n",
    "- Small models (ResNet18): 16-32GB\n",
    "- Medium models (ResNet50): 32-64GB  \n",
    "- Large models (Transformers): 64-128GB\n",
    "\n",
    "Time Guidelines:\n",
    "- Quick experiments: 1-4 hours\n",
    "- Standard training: 8-24 hours\n",
    "- Large experiments: 24-72 hours\n",
    "\n",
    "CPU Guidelines:\n",
    "- 2-4 CPUs per GPU for data loading\n",
    "- More CPUs for heavy preprocessing\n",
    "\"\"\"\n",
    "\n",
    "print(resource_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Job Submission and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job management commands\n",
    "management_commands = \"\"\"\n",
    "# Submit job\n",
    "sbatch job_script.sh\n",
    "\n",
    "# Check job status\n",
    "squeue -u $USER\n",
    "\n",
    "# Check job details\n",
    "scontrol show job <JOBID>\n",
    "\n",
    "# Cancel job\n",
    "scancel <JOBID>\n",
    "\n",
    "# Cancel all your jobs\n",
    "scancel -u $USER\n",
    "\n",
    "# Check job history\n",
    "sacct -u $USER --starttime=today\n",
    "\n",
    "# Check job efficiency\n",
    "seff <JOBID>\n",
    "\"\"\"\n",
    "\n",
    "print(management_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Example Training Script Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Python training script that works well with sbatch\n",
    "training_script_example = '''\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--experiment-name', type=str, default='experiment')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"cluster-training\",\n",
    "        name=args.experiment_name,\n",
    "        config=args\n",
    "    )\n",
    "    \n",
    "    # Check GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Your training code here\n",
    "    # ...\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'model_{args.experiment_name}.pth')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('train_model.py', 'w') as f:\n",
    "    f.write(training_script_example)\n",
    "\n",
    "print(\"Created example train_model.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "### Common SBATCH Directives\n",
    "\n",
    "| Directive | Purpose | Example |\n",
    "|-----------|---------|----------|\n",
    "| `--job-name` | Job name | `--job-name=my_training` |\n",
    "| `--partition` | Queue/partition | `--partition=gpu` |\n",
    "| `--gres` | GPU resources | `--gres=gpu:1` |\n",
    "| `--mem` | Memory | `--mem=32G` |\n",
    "| `--time` | Time limit | `--time=08:00:00` |\n",
    "| `--output` | Output file | `--output=job_%j.out` |\n",
    "| `--error` | Error file | `--error=job_%j.err` |\n",
    "| `--mail-type` | Email notifications | `--mail-type=END,FAIL` |\n",
    "| `--array` | Job array | `--array=1-10` |\n",
    "\n",
    "### Job Submission Workflow\n",
    "\n",
    "1. **Create job script** with SBATCH directives\n",
    "2. **Test locally** with small dataset first\n",
    "3. **Submit job** with `sbatch script.sh`\n",
    "4. **Monitor progress** with `squeue -u $USER`\n",
    "5. **Check outputs** in `.out` and `.err` files\n",
    "6. **Analyze results** and iterate\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Try submitting a simple job\n",
    "- Monitor job progress\n",
    "- Check the job monitoring guide for advanced management\n",
    "- Scale up to job arrays for parameter sweeps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
