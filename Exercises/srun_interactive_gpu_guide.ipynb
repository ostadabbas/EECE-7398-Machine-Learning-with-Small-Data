{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive GPU Computing with srun on Explorer Cluster\n",
    "\n",
    "This notebook provides a comprehensive guide to using `srun` for interactive GPU computing on Northeastern's Explorer cluster. You'll learn how to request GPU resources, monitor usage, and optimize your interactive sessions.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Understanding srun](#understanding-srun)\n",
    "2. [Basic GPU Requests](#basic-gpu-requests)\n",
    "3. [Advanced Resource Configuration](#advanced-resource-configuration)\n",
    "4. [Monitoring Your Session](#monitoring-session)\n",
    "5. [Best Practices](#best-practices)\n",
    "6. [Common Use Cases](#common-use-cases)\n",
    "7. [Troubleshooting](#troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding srun <a name=\"understanding-srun\"></a>\n",
    "\n",
    "The `srun` command is used to submit **interactive jobs** to the SLURM scheduler. Unlike batch jobs, interactive jobs give you a shell session on compute nodes where you can run commands in real-time.\n",
    "\n",
    "### Key Benefits:\n",
    "- Real-time interaction with compute nodes\n",
    "- Perfect for development, debugging, and testing\n",
    "- Immediate feedback from your code\n",
    "- Ideal for Jupyter notebooks and interactive Python sessions\n",
    "\n",
    "### Basic Syntax:\n",
    "```bash\n",
    "srun [options] [command]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic GPU Requests <a name=\"basic-gpu-requests\"></a>\n",
    "\n",
    "### 2.1 Simple GPU Request\n",
    "\n",
    "The most basic way to request a GPU for interactive use:\n",
    "\n",
    "```bash\n",
    "srun --partition=gpu-interactive --nodes=1 --ntasks=1 --gres=gpu:1 --mem=8G --time=01:00:00 --pty /bin/bash\n",
    "```\n",
    "\n",
    "**Breakdown:**\n",
    "- `--partition=gpu-interactive`: Use the interactive GPU partition\n",
    "- `--nodes=1`: Request 1 compute node\n",
    "- `--ntasks=1`: Run 1 task\n",
    "- `--gres=gpu:1`: Request 1 GPU\n",
    "- `--mem=8G`: Request 8GB of memory\n",
    "- `--time=01:00:00`: Request 1 hour of time\n",
    "- `--pty /bin/bash`: Start an interactive bash shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Quick GPU Session\n",
    "\n",
    "For quick testing (15 minutes):\n",
    "\n",
    "```bash\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --time=00:15:00 --pty bash\n",
    "```\n",
    "\n",
    "### 2.3 Extended Development Session\n",
    "\n",
    "For longer development work (4 hours with more resources):\n",
    "\n",
    "```bash\n",
    "srun --partition=gpu-interactive --nodes=1 --ntasks=1 --gres=gpu:1 --mem=32G --cpus-per-task=4 --time=04:00:00 --pty /bin/bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Resource Configuration <a name=\"advanced-resource-configuration\"></a>\n",
    "\n",
    "### 3.1 Multiple GPUs\n",
    "\n",
    "Request multiple GPUs for multi-GPU training:\n",
    "\n",
    "```bash\n",
    "# Request 2 GPUs\n",
    "srun --partition=gpu-interactive --gres=gpu:2 --mem=64G --cpus-per-task=8 --time=02:00:00 --pty bash\n",
    "\n",
    "# Request 4 GPUs\n",
    "srun --partition=gpu-interactive --gres=gpu:4 --mem=128G --cpus-per-task=16 --time=02:00:00 --pty bash\n",
    "```\n",
    "\n",
    "### 3.2 Specific GPU Types\n",
    "\n",
    "Request specific GPU models if available:\n",
    "\n",
    "```bash\n",
    "# Request specific GPU type (if available)\n",
    "srun --partition=gpu-interactive --gres=gpu:a100:1 --mem=32G --time=02:00:00 --pty bash\n",
    "srun --partition=gpu-interactive --gres=gpu:v100:1 --mem=32G --time=02:00:00 --pty bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Memory and CPU Configuration\n",
    "\n",
    "Different memory configurations for different workloads:\n",
    "\n",
    "```bash\n",
    "# Light workload (small models, testing)\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=16G --cpus-per-task=2 --time=01:00:00 --pty bash\n",
    "\n",
    "# Medium workload (standard training)\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=32G --cpus-per-task=4 --time=02:00:00 --pty bash\n",
    "\n",
    "# Heavy workload (large models, data processing)\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=64G --cpus-per-task=8 --time=04:00:00 --pty bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitoring Your Session <a name=\"monitoring-session\"></a>\n",
    "\n",
    "### 4.1 Check Your Job Status\n",
    "\n",
    "Once you submit an srun request, you can monitor it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a terminal to check your jobs\n",
    "# squeue -u $USER\n",
    "\n",
    "# Example output explanation\n",
    "print(\"\"\"\n",
    "JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "12345 gpu-inter     bash username  R       5:23      1 gpu-node-01\n",
    "\n",
    "Status meanings:\n",
    "- R: Running\n",
    "- PD: Pending (waiting for resources)\n",
    "- CG: Completing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Monitor GPU Usage\n",
    "\n",
    "Once you're in your interactive session, monitor GPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to run in your srun session\n",
    "print(\"\"\"\n",
    "# Check GPU status\n",
    "nvidia-smi\n",
    "\n",
    "# Monitor GPU usage continuously (update every 2 seconds)\n",
    "watch -n 2 nvidia-smi\n",
    "\n",
    "# Check GPU memory usage\n",
    "nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Python GPU Monitoring\n",
    "\n",
    "Monitor GPU usage from within Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def check_gpu_status():\n",
    "    \"\"\"Check GPU availability and memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "        print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3  # GB\n",
    "        memory_cached = torch.cuda.memory_reserved(0) / 1024**3  # GB\n",
    "        print(f\"Memory Allocated: {memory_allocated:.2f} GB\")\n",
    "        print(f\"Memory Cached: {memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "# Run the check\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices <a name=\"best-practices\"></a>\n",
    "\n",
    "### 5.1 Resource Optimization\n",
    "\n",
    "- **Start small**: Begin with minimal resources and scale up as needed\n",
    "- **Time limits**: Request only the time you actually need\n",
    "- **Memory**: Don't over-request memory - it affects queue times\n",
    "- **CPUs**: Match CPU count to your workload (typically 2-4 CPUs per GPU)\n",
    "\n",
    "### 5.2 Queue Time Optimization\n",
    "\n",
    "```bash\n",
    "# Check queue status before submitting\n",
    "sinfo -p gpu-interactive\n",
    "\n",
    "# Check estimated start time\n",
    "squeue -u $USER --start\n",
    "```\n",
    "\n",
    "### 5.3 Session Management\n",
    "\n",
    "- Use `screen` or `tmux` for persistent sessions\n",
    "- Save your work frequently\n",
    "- Clean up GPU memory when switching tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU memory cleanup in Python\n",
    "import torch\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU cache cleared\")\n",
    "    else:\n",
    "        print(\"No GPU to clean up\")\n",
    "\n",
    "# Use this between different experiments\n",
    "cleanup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common Use Cases <a name=\"common-use-cases\"></a>\n",
    "\n",
    "### 6.1 Jupyter Notebook Session\n",
    "\n",
    "Start a GPU session for Jupyter notebooks:\n",
    "\n",
    "```bash\n",
    "# Request GPU resources\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=32G --time=03:00:00 --pty bash\n",
    "\n",
    "# Once in the session, load your environment and start Jupyter\n",
    "module load anaconda3\n",
    "conda activate ml_course_env\n",
    "jupyter notebook --no-browser --port=8888\n",
    "```\n",
    "\n",
    "### 6.2 Interactive Python Development\n",
    "\n",
    "```bash\n",
    "# Start interactive session\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=16G --time=02:00:00 --pty bash\n",
    "\n",
    "# Load environment and start Python\n",
    "module load anaconda3\n",
    "conda activate ml_course_env\n",
    "python\n",
    "```\n",
    "\n",
    "### 6.3 Model Testing and Debugging\n",
    "\n",
    "```bash\n",
    "# Quick testing session\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=16G --time=01:00:00 --pty bash\n",
    "\n",
    "# Run your test script\n",
    "python test_model.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting <a name=\"troubleshooting\"></a>\n",
    "\n",
    "### 7.1 Common Issues and Solutions\n",
    "\n",
    "**Issue: Job stays in pending (PD) state**\n",
    "```bash\n",
    "# Check why job is pending\n",
    "squeue -u $USER\n",
    "squeue -j <JOBID> --start\n",
    "\n",
    "# Common reasons:\n",
    "# - Resources: Requested resources not available\n",
    "# - Priority: Other jobs have higher priority\n",
    "# - Limits: Hit partition or user limits\n",
    "```\n",
    "\n",
    "**Issue: Out of memory errors**\n",
    "```bash\n",
    "# Request more memory\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=64G --time=02:00:00 --pty bash\n",
    "\n",
    "# Or optimize your code to use less memory\n",
    "```\n",
    "\n",
    "**Issue: Session disconnected**\n",
    "```bash\n",
    "# Use screen or tmux for persistent sessions\n",
    "srun --partition=gpu-interactive --gres=gpu:1 --mem=32G --time=02:00:00 --pty bash\n",
    "screen -S gpu_session\n",
    "# Your work here\n",
    "# Ctrl+A, D to detach\n",
    "# screen -r gpu_session to reattach\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Useful Commands for Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to run in terminal for troubleshooting\n",
    "troubleshooting_commands = \"\"\"\n",
    "# Check partition information\n",
    "sinfo -p gpu-interactive\n",
    "\n",
    "# Check your job queue\n",
    "squeue -u $USER\n",
    "\n",
    "# Check job details\n",
    "scontrol show job <JOBID>\n",
    "\n",
    "# Check node information\n",
    "sinfo -N -l\n",
    "\n",
    "# Check your account limits\n",
    "sacctmgr show assoc user=$USER\n",
    "\n",
    "# Cancel a job if needed\n",
    "scancel <JOBID>\n",
    "\n",
    "# Check completed jobs\n",
    "sacct -u $USER --starttime=today\n",
    "\"\"\"\n",
    "\n",
    "print(troubleshooting_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference Card\n",
    "\n",
    "### Essential srun Commands\n",
    "\n",
    "| Purpose | Command |\n",
    "|---------|----------|\n",
    "| Quick GPU test | `srun --partition=gpu-interactive --gres=gpu:1 --time=00:15:00 --pty bash` |\n",
    "| Standard development | `srun --partition=gpu-interactive --gres=gpu:1 --mem=32G --time=02:00:00 --pty bash` |\n",
    "| Heavy workload | `srun --partition=gpu-interactive --gres=gpu:1 --mem=64G --cpus-per-task=8 --time=04:00:00 --pty bash` |\n",
    "| Multi-GPU | `srun --partition=gpu-interactive --gres=gpu:2 --mem=64G --time=02:00:00 --pty bash` |\n",
    "\n",
    "### Monitoring Commands\n",
    "\n",
    "| Purpose | Command |\n",
    "|---------|----------|\n",
    "| Check your jobs | `squeue -u $USER` |\n",
    "| GPU status | `nvidia-smi` |\n",
    "| Partition info | `sinfo -p gpu-interactive` |\n",
    "| Cancel job | `scancel <JOBID>` |\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "- Try the basic GPU request examples above\n",
    "- Monitor your resource usage\n",
    "- Move to batch jobs (`sbatch`) for longer-running tasks\n",
    "- Check out the job monitoring guide for advanced management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
