{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive PyTorch GPU Training Tutorial\n",
    "\n",
    "This notebook will guide you through using PyTorch with GPU acceleration. It includes interactive examples and real-time performance comparisons.\n",
    "\n",
    "## Table of Contents\n",
    "1. [GPU Setup and Verification](#gpu-setup)\n",
    "2. [Basic GPU Operations](#basic-operations)\n",
    "3. [Performance Comparison](#performance)\n",
    "4. [Memory Management](#memory)\n",
    "5. [Multi-GPU Training](#multi-gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Setup and Verification <a name=\"gpu-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "def check_gpu_availability():\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Python version: {sys.version.split()[0]}\")\n",
    "    print(f\"Operating System: {platform.system()} {platform.version()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\n✅ CUDA is available!\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        # Get memory info\n",
    "        memory_allocated = torch.cuda.memory_allocated(0)\n",
    "        memory_reserved = torch.cuda.memory_reserved(0)\n",
    "        print(f\"\\nGPU Memory:\")\n",
    "        print(f\"- Allocated: {memory_allocated/1024**2:.2f} MB\")\n",
    "        print(f\"- Reserved:  {memory_reserved/1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"\\n❌ CUDA is not available. Running on CPU only.\")\n",
    "\n",
    "check_gpu_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic GPU Operations <a name=\"basic-operations\"></a>\n",
    "\n",
    "Let's explore how to move tensors and operations to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gpu_operations():\n",
    "    # Create tensors\n",
    "    cpu_tensor = torch.randn(1000, 1000)\n",
    "    \n",
    "    print(\"1. Creating tensors:\")\n",
    "    print(f\"CPU tensor device: {cpu_tensor.device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Move to GPU\n",
    "        gpu_tensor = cpu_tensor.cuda()\n",
    "        print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "        \n",
    "        # Create tensor directly on GPU\n",
    "        direct_gpu_tensor = torch.randn(1000, 1000, device='cuda')\n",
    "        print(f\"Direct GPU tensor device: {direct_gpu_tensor.device}\")\n",
    "        \n",
    "        # Basic operations\n",
    "        print(\"\\n2. Basic operations on GPU:\")\n",
    "        result = gpu_tensor @ direct_gpu_tensor\n",
    "        print(f\"Matrix multiplication result device: {result.device}\")\n",
    "        \n",
    "        # Moving back to CPU\n",
    "        print(\"\\n3. Moving back to CPU:\")\n",
    "        cpu_result = result.cpu()\n",
    "        print(f\"Result moved back to CPU device: {cpu_result.device}\")\n",
    "    else:\n",
    "        print(\"GPU operations not available\")\n",
    "\n",
    "demonstrate_gpu_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Comparison <a name=\"performance\"></a>\n",
    "\n",
    "Let's compare the performance of CPU vs GPU for common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_operations():\n",
    "    sizes = [1000, 2000, 4000]\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        # CPU timing\n",
    "        a_cpu = torch.randn(size, size)\n",
    "        b_cpu = torch.randn(size, size)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = torch.matmul(a_cpu, b_cpu)\n",
    "        cpu_time = time.time() - start_time\n",
    "        \n",
    "        # GPU timing\n",
    "        if torch.cuda.is_available():\n",
    "            a_gpu = a_cpu.cuda()\n",
    "            b_gpu = b_cpu.cuda()\n",
    "            \n",
    "            # Warm-up\n",
    "            _ = torch.matmul(a_gpu, b_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = torch.matmul(a_gpu, b_gpu)\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_time = time.time() - start_time\n",
    "        else:\n",
    "            gpu_time = float('nan')\n",
    "            \n",
    "        results.append({\n",
    "            'size': size,\n",
    "            'cpu_time': cpu_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'speedup': cpu_time/gpu_time if gpu_time > 0 else float('nan')\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Matrix Multiplication Performance Comparison:\")\n",
    "    print(\"\\nSize\\t\\tCPU (s)\\t\\tGPU (s)\\t\\tSpeedup\")\n",
    "    print(\"-\" * 60)\n",
    "    for r in results:\n",
    "        print(f\"{r['size']}x{r['size']}\\t{r['cpu_time']:.4f}\\t\\t{r['gpu_time']:.4f}\\t\\t{r['speedup']:.2f}x\")\n",
    "\n",
    "benchmark_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Management <a name=\"memory\"></a>\n",
    "\n",
    "Understanding GPU memory management is crucial for efficient deep learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_memory_management():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"GPU not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"Initial GPU Memory Usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    print(f\"Cached:    {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Create some tensors\n",
    "    tensors = []\n",
    "    for i in range(5):\n",
    "        tensors.append(torch.randn(1000, 1000, device='cuda'))\n",
    "        print(f\"\\nAfter creating tensor {i+1}:\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "        print(f\"Cached:    {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Clear some memory\n",
    "    print(\"\\nClearing tensors...\")\n",
    "    del tensors\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\nFinal GPU Memory Usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    print(f\"Cached:    {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "\n",
    "demonstrate_memory_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-GPU Training <a name=\"multi-gpu\"></a>\n",
    "\n",
    "If multiple GPUs are available, let's see how to use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multi_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"GPU not available\")\n",
    "        return\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    if num_gpus > 1:\n",
    "        print(\"\\nGPU Details:\")\n",
    "        for i in range(num_gpus):\n",
    "            print(f\"\\nGPU {i}:\")\n",
    "            print(f\"Name: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Memory Allocated: {torch.cuda.memory_allocated(i)/1024**2:.2f} MB\")\n",
    "            print(f\"Memory Cached: {torch.cuda.memory_reserved(i)/1024**2:.2f} MB\")\n",
    "            \n",
    "        # Example of DataParallel\n",
    "        model = torch.nn.Linear(100, 10)\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        print(f\"\\nModel using DataParallel: {model.device_ids}\")\n",
    "    else:\n",
    "        print(\"Multi-GPU training not available (only one GPU detected)\")\n",
    "\n",
    "check_multi_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "1. **Memory Management**:\n",
    "   - Use `del` to remove unused tensors\n",
    "   - Call `torch.cuda.empty_cache()` to free memory\n",
    "   - Monitor memory usage with `torch.cuda.memory_summary()`\n",
    "\n",
    "2. **Performance Optimization**:\n",
    "   - Use larger batch sizes on GPU\n",
    "   - Minimize CPU-GPU data transfers\n",
    "   - Use `torch.cuda.synchronize()` for accurate timing\n",
    "\n",
    "3. **Multi-GPU Training**:\n",
    "   - Use `DataParallel` for simple multi-GPU training\n",
    "   - Consider `DistributedDataParallel` for better performance\n",
    "   - Balance load across GPUs\n",
    "\n",
    "4. **Common Pitfalls**:\n",
    "   - Check tensor device before operations\n",
    "   - Watch out for memory leaks\n",
    "   - Be aware of synchronization points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
